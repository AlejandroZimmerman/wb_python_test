{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Research Assistant Python Test**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import packages needed for the test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Import packages\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import regex as re\n",
    "from collections import Counter\n",
    "from tqdm import tqdm\n",
    "from rapidfuzz import process, fuzz\n",
    "\n",
    "# ML packages\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# Python version : 3.11.9\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Import data\n",
    "# Set the directory path\n",
    "directory_path = r\"C:\\Users\\alezi\\OneDrive\\Escritorio\\Test WB\\Python\"\n",
    "# directory_path = r\"WB_Path\"\n",
    "\n",
    "# Change the current working directory\n",
    "os.chdir(directory_path)\n",
    "\n",
    "# Load the data\n",
    "names_2019_2020 = pd.read_csv(\"data/ForeignNames_2019_2020.csv\")\n",
    "country_iso = pd.read_csv(\"data/Country_Name_ISO3.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merge Results (Value Counts):\n",
      "_merge\n",
      "both          594032\n",
      "left_only      29048\n",
      "right_only         0\n",
      "Name: count, dtype: int64\n",
      "\n",
      "\n",
      "Countries that did not merge (left_only):\n",
      "foreigncountry_cleaned\n",
      "South Korea                              27791\n",
      "Iran                                       656\n",
      "Tanzania                                   479\n",
      "Congo, the Democratic Republic of the       98\n",
      "Antigua And Barbuda                         24\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Merge the foreign names dataset with the country ISO3 codes\n",
    "name_country_data = pd.merge(names_2019_2020 , country_iso, \n",
    "                     left_on='foreigncountry_cleaned', \n",
    "                     right_on='country_name', \n",
    "                     how='left', indicator=True)\n",
    "\n",
    "# Check the merge results\n",
    "merge_result_counts = name_country_data['_merge'].value_counts()\n",
    "print(\"Merge Results (Value Counts):\")\n",
    "print(merge_result_counts)\n",
    "print(\"\\n\") \n",
    "# There are observations that did not merge (left_only)\n",
    "\n",
    "# Check the countries that did not merge(South Korea, North Korea, Iran, Congo, the Democratic Republic of the, Antigua And Barbuda)\n",
    "print(\"Countries that did not merge (left_only):\")\n",
    "unmerged_countries = name_country_data[name_country_data['_merge'] == 'left_only']['foreigncountry_cleaned'].value_counts()\n",
    "print(unmerged_countries)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merge Results (Value Counts):\n",
      "_merge\n",
      "both          623362\n",
      "left_only          0\n",
      "right_only         0\n",
      "Name: count, dtype: int64\n",
      "\n",
      "\n",
      "Merge was successful\n"
     ]
    }
   ],
   "source": [
    "## Fixing the Merge \n",
    "\n",
    "# Fix Congo in the names_2019_2020 dataset\n",
    "names_2019_2020['foreigncountry_cleaned'] = names_2019_2020['foreigncountry_cleaned'].replace('Congo, the Democratic Republic of the','Congo')\n",
    "\n",
    "# Manual mapping of problematic country names\n",
    "manual_country_mapping = {\n",
    "    'Korea, Republic of': 'South Korea',\n",
    "    'Iran, Islamic Republic of': 'Iran', \n",
    "    'Congo, The Democratic Republic of the': 'Congo',\n",
    "    'Antigua and Barbuda': 'Antigua And Barbuda',\n",
    "    'Tanzania, United Republic of': 'Tanzania',\n",
    "}\n",
    "\n",
    "# Apply the manual mapping to the 'foreigncountry_cleaned' column\n",
    "country_iso['name_clean'] = country_iso['country_name'].replace(manual_country_mapping)\n",
    "\n",
    "# Now we can merge based on the mapped country names\n",
    "name_country_data = pd.merge(names_2019_2020, country_iso, \n",
    "                             left_on='foreigncountry_cleaned', \n",
    "                             right_on='name_clean', \n",
    "                             how='left', indicator=True)\n",
    "\n",
    "# Check the results\n",
    "merge_result_counts = name_country_data['_merge'].value_counts()\n",
    "print(\"Merge Results (Value Counts):\")\n",
    "print(merge_result_counts)\n",
    "\n",
    "# Print that if there are no left_only values (left_only = 0), the merge was successful\n",
    "if merge_result_counts['left_only'] == 0:\n",
    "    print(\"\\n\")\n",
    "    print(\"Merge was successful\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Cleaning and Matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## STOPWORDS PER COUNTRY ##\n",
    "\n",
    "'''\n",
    "Define stopwords for cleaning, customized per country.\n",
    "\n",
    "To enhance the accuracy of cleaning and matching company names, I process the dataset country by country.\n",
    "For each country, I select the top 10 most frequent words found in the company names specific to that country.\n",
    "These words are added to a dictionary called 'countries_stopwords', where each key is a country ISO3 code, \n",
    "and the value is the list of stopwords for that country.\n",
    "\n",
    "As I analyze more data, I can expand and refine the stopword lists for each country to further enhance accuracy.\n",
    "'''\n",
    "\n",
    "\n",
    "# Initialize an empty dictionary to hold stopwords per country\n",
    "countries_stopwords = {}\n",
    "\n",
    "# Group the data by 'country_iso3'\n",
    "grouped = name_country_data.groupby('country_iso3')\n",
    "\n",
    "# Iterate over each country group\n",
    "for country_code, group in grouped:\n",
    "    # Get the list of company names for this country, ensuring no NaN values\n",
    "    company_names = group['foreign'].dropna().tolist()\n",
    "    \n",
    "    # Initialize a list to hold all words\n",
    "    all_words = []\n",
    "    \n",
    "    # For each company name, extract words\n",
    "    for name in company_names:\n",
    "        # Convert to lowercase and remove non-alphabetic characters\n",
    "        name = name.lower()\n",
    "        name = re.sub(r'[^a-z\\s]', '', name)\n",
    "        # Split into words\n",
    "        words = name.split()\n",
    "        # Add words to the list\n",
    "        all_words.extend(words)\n",
    "    \n",
    "    # Count the frequency of each word\n",
    "    word_counts = Counter(all_words)\n",
    "    # Get the top 8 most common words\n",
    "    top_words = [word for word, count in word_counts.most_common(8)]\n",
    "    # Add these words to the countries_stopwords dictionary\n",
    "    countries_stopwords[country_code] = top_words\n",
    "    \n",
    "# Add manuallly defined stopwords \n",
    "countries_stopwords['USA'].append('technologies')\n",
    "countries_stopwords['CHN'].append('machinery')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are no missing values in the 'foreign' column\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\alezi\\AppData\\Local\\Temp\\ipykernel_34596\\3045459439.py:23: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  name_country_data['cleaned_name'] = name_country_data.apply(\n"
     ]
    }
   ],
   "source": [
    "# Define the function to clean the company names\n",
    "def clean_company_name(company_name, stopwords):\n",
    "    if not isinstance(company_name, str):\n",
    "        return ''\n",
    "    # Convert to lowercase\n",
    "    company_name = company_name.lower()\n",
    "    # Remove non-alphabetic characters\n",
    "    company_name = re.sub(r'[^a-z\\s]', '', company_name)\n",
    "    # Split into words\n",
    "    words = company_name.split()\n",
    "    # Remove stopwords\n",
    "    cleaned_words = [word for word in words if word not in stopwords]\n",
    "    # Reconstruct the cleaned name\n",
    "    cleaned_name = ' '.join(cleaned_words)\n",
    "    return cleaned_name\n",
    "\n",
    "# Drop the rows where the company name is missing\n",
    "name_country_data = name_country_data.dropna(subset=['foreign'])\n",
    "if name_country_data['foreign'].isnull().sum() == 0:\n",
    "    print(\"There are no missing values in the 'foreign' column\")\n",
    "\n",
    "# Apply the cleaning function to the DataFrame using country-specific stopwords\n",
    "name_country_data['cleaned_name'] = name_country_data.apply(\n",
    "    lambda row: clean_company_name(\n",
    "        row['foreign'],\n",
    "        countries_stopwords.get(row['country_iso3'], [])\n",
    "    ),\n",
    "    axis=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "## RapidFuzz Functions ##\n",
    "\n",
    "# Function to apply fuzzy matching within the same country using RapidFuzz\n",
    "def fuzzy_match_within_country(df, similarity_threshold=85, min_length=4):\n",
    "    # Use unique cleaned names to avoid duplicates\n",
    "    unique_cleaned_names = list(set(df['cleaned_name'].tolist()))\n",
    "    name_to_group = {}\n",
    "    group_id = 0\n",
    "    processed = set()\n",
    "\n",
    "    for name in tqdm(unique_cleaned_names, desc=\"Fuzzy Matching Progress\"):\n",
    "        if name in processed:\n",
    "            continue\n",
    "\n",
    "        # Assign unique group_id to short names (in this way I avoid that all short names are assigned to the same group)\n",
    "        if len(name) < min_length:\n",
    "            name_to_group[name] = group_id\n",
    "            processed.add(name)\n",
    "            group_id += 1\n",
    "            continue\n",
    "\n",
    "        # Use token_sort_ratio for matching longer names and prevent false positives (e.g., \"tti\" matches \"patiperro\" in DEU)\n",
    "        matches = process.extract(\n",
    "            name,\n",
    "            unique_cleaned_names,\n",
    "            scorer=fuzz.token_sort_ratio,\n",
    "            score_cutoff=similarity_threshold,\n",
    "            limit=None\n",
    "        )\n",
    "\n",
    "        similar_names = [match[0] for match in matches if len(match[0]) >= min_length] # Filter out short names\n",
    "\n",
    "        if not similar_names:\n",
    "            name_to_group[name] = group_id\n",
    "            processed.add(name)\n",
    "            group_id += 1\n",
    "            continue\n",
    "\n",
    "        # Assign group_id to all similar names\n",
    "        for sim_name in similar_names:\n",
    "            if sim_name not in name_to_group:\n",
    "                name_to_group[sim_name] = group_id\n",
    "                processed.add(sim_name)\n",
    "        group_id += 1\n",
    "\n",
    "    # Map group_id back to the DataFrame\n",
    "    df['group_id'] = df['cleaned_name'].map(name_to_group)\n",
    "    return df\n",
    "\n",
    "\n",
    "# Function to assign cleaned_ID based on group_id and country code\n",
    "def assign_cleaned_id(df, country_code):\n",
    "    df['cleaned_ID'] = country_code + '_' + df['group_id'].astype(str)\n",
    "    return df\n",
    "\n",
    "def process_all_countries(df, similarity_threshold=85):\n",
    "    final_dfs = []\n",
    "    countries = df['country_iso3'].unique()\n",
    "    \n",
    "    for country in tqdm(countries, desc=\"Processing Countries\"):\n",
    "        country_df = df[df['country_iso3'] == country].reset_index(drop=True)\n",
    "        if country_df.empty:\n",
    "            continue\n",
    "        # Apply fuzzy matching within the country\n",
    "        matched_df = fuzzy_match_within_country(country_df, similarity_threshold)\n",
    "        # Assign cleaned IDs\n",
    "        assigned_df = assign_cleaned_id(matched_df, country)\n",
    "        final_dfs.append(assigned_df)\n",
    "    \n",
    "    # Combine all country DataFrames\n",
    "    final_df = pd.concat(final_dfs, ignore_index=True)\n",
    "    return final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fuzzy Matching Progress: 100%|██████████| 22245/22245 [09:04<00:00, 40.84it/s]\n",
      "Fuzzy Matching Progress: 100%|██████████| 533/533 [00:00<00:00, 963.65it/s] \n",
      "Fuzzy Matching Progress: 100%|██████████| 35951/35951 [21:54<00:00, 27.36it/s]\n",
      "Fuzzy Matching Progress: 100%|██████████| 3193/3193 [00:06<00:00, 465.17it/s]\n",
      "Fuzzy Matching Progress: 100%|██████████| 1827/1827 [00:02<00:00, 791.72it/s]\n",
      "Fuzzy Matching Progress: 100%|██████████| 16128/16128 [03:12<00:00, 83.81it/s]\n",
      "Fuzzy Matching Progress: 100%|██████████| 9422/9422 [01:16<00:00, 123.29it/s]\n",
      "Fuzzy Matching Progress: 100%|██████████| 5961/5961 [00:30<00:00, 198.08it/s]\n",
      "Fuzzy Matching Progress: 100%|██████████| 16400/16400 [04:07<00:00, 66.22it/s]\n",
      "Fuzzy Matching Progress: 100%|██████████| 732/732 [00:00<00:00, 1714.91it/s]\n",
      "Fuzzy Matching Progress: 100%|██████████| 8834/8834 [01:06<00:00, 133.65it/s]\n",
      "Fuzzy Matching Progress: 100%|██████████| 14048/14048 [02:49<00:00, 82.70it/s]\n",
      "Fuzzy Matching Progress: 100%|██████████| 1051/1051 [00:00<00:00, 1155.52it/s]\n",
      "Fuzzy Matching Progress: 100%|██████████| 18272/18272 [03:58<00:00, 76.48it/s]\n",
      "Fuzzy Matching Progress: 100%|██████████| 2586/2586 [00:04<00:00, 525.49it/s]\n",
      "Fuzzy Matching Progress: 100%|██████████| 6335/6335 [00:31<00:00, 198.45it/s]\n",
      "Fuzzy Matching Progress: 100%|██████████| 12010/12010 [02:30<00:00, 79.98it/s]\n",
      "Fuzzy Matching Progress: 100%|██████████| 9084/9084 [01:01<00:00, 148.57it/s]\n",
      "Fuzzy Matching Progress: 100%|██████████| 15420/15420 [03:31<00:00, 72.91it/s]\n",
      "Fuzzy Matching Progress: 100%|██████████| 1775/1775 [00:02<00:00, 877.78it/s]\n",
      "Fuzzy Matching Progress: 100%|██████████| 814/814 [00:00<00:00, 2120.86it/s]\n",
      "Fuzzy Matching Progress: 100%|██████████| 1289/1289 [00:00<00:00, 1315.54it/s]\n",
      "Fuzzy Matching Progress: 100%|██████████| 4077/4077 [00:14<00:00, 274.26it/s]\n",
      "Fuzzy Matching Progress: 100%|██████████| 19731/19731 [06:01<00:00, 54.62it/s]\n",
      "Fuzzy Matching Progress: 100%|██████████| 10864/10864 [01:42<00:00, 106.14it/s]\n",
      "Fuzzy Matching Progress: 100%|██████████| 14786/14786 [03:26<00:00, 71.66it/s]\n",
      "Fuzzy Matching Progress: 100%|██████████| 97293/97293 [3:18:16<00:00,  8.18it/s]\n",
      "Fuzzy Matching Progress: 100%|██████████| 3997/3997 [00:15<00:00, 254.75it/s]]\n",
      "Fuzzy Matching Progress: 100%|██████████| 729/729 [00:00<00:00, 1360.66it/s]t]\n",
      "Fuzzy Matching Progress: 100%|██████████| 203/203 [00:00<00:00, 4405.14it/s]t]\n",
      "Fuzzy Matching Progress: 100%|██████████| 63/63 [00:00<00:00, 15861.77it/s]\n",
      "Fuzzy Matching Progress: 100%|██████████| 1592/1592 [00:02<00:00, 616.52it/s] \n",
      "Fuzzy Matching Progress: 100%|██████████| 1524/1524 [00:02<00:00, 738.57it/s]\n",
      "Fuzzy Matching Progress: 100%|██████████| 1563/1563 [00:02<00:00, 742.14it/s]\n",
      "Fuzzy Matching Progress: 100%|██████████| 1334/1334 [00:01<00:00, 863.05it/s]\n",
      "Fuzzy Matching Progress: 100%|██████████| 162/162 [00:00<00:00, 5883.28it/s]\n",
      "Fuzzy Matching Progress: 100%|██████████| 511/511 [00:00<00:00, 2304.86it/s]\n",
      "Fuzzy Matching Progress: 100%|██████████| 592/592 [00:00<00:00, 1724.99it/s]\n",
      "Fuzzy Matching Progress: 100%|██████████| 584/584 [00:00<00:00, 1892.77it/s]\n",
      "Fuzzy Matching Progress: 100%|██████████| 4529/4529 [00:27<00:00, 165.60it/s]\n",
      "Fuzzy Matching Progress: 100%|██████████| 85/85 [00:00<00:00, 15147.68it/s]\n",
      "Fuzzy Matching Progress: 100%|██████████| 4195/4195 [00:17<00:00, 234.56it/s]\n",
      "Fuzzy Matching Progress: 100%|██████████| 258/258 [00:00<00:00, 4812.66it/s]\n",
      "Fuzzy Matching Progress: 100%|██████████| 1926/1926 [00:03<00:00, 513.59it/s]\n",
      "Fuzzy Matching Progress: 100%|██████████| 5193/5193 [00:29<00:00, 175.53it/s]\n",
      "Fuzzy Matching Progress: 100%|██████████| 692/692 [00:00<00:00, 1742.91it/s]\n",
      "Fuzzy Matching Progress: 100%|██████████| 761/761 [00:00<00:00, 1311.06it/s]\n",
      "Fuzzy Matching Progress: 100%|██████████| 255/255 [00:00<00:00, 5068.75it/s]\n",
      "Fuzzy Matching Progress: 100%|██████████| 192/192 [00:00<00:00, 5387.39it/s]\n",
      "Fuzzy Matching Progress: 100%|██████████| 316/316 [00:00<00:00, 3507.79it/s]\n",
      "Fuzzy Matching Progress: 100%|██████████| 404/404 [00:00<00:00, 2692.02it/s]\n",
      "Fuzzy Matching Progress: 100%|██████████| 389/389 [00:00<00:00, 3322.65it/s]\n",
      "Fuzzy Matching Progress: 100%|██████████| 854/854 [00:00<00:00, 1333.27it/s]\n",
      "Fuzzy Matching Progress: 100%|██████████| 591/591 [00:00<00:00, 2024.81it/s]\n",
      "Fuzzy Matching Progress: 100%|██████████| 2422/2422 [00:06<00:00, 393.35it/s]\n",
      "Fuzzy Matching Progress: 100%|██████████| 60/60 [00:00<00:00, 23829.02it/s]\n",
      "Fuzzy Matching Progress: 100%|██████████| 2524/2524 [00:06<00:00, 413.08it/s]\n",
      "Fuzzy Matching Progress: 100%|██████████| 369/369 [00:00<00:00, 2653.64it/s]\n",
      "Fuzzy Matching Progress: 100%|██████████| 1069/1069 [00:01<00:00, 994.57it/s] \n",
      "Fuzzy Matching Progress: 100%|██████████| 492/492 [00:00<00:00, 2198.53it/s]\n",
      "Fuzzy Matching Progress: 100%|██████████| 411/411 [00:00<00:00, 2390.67it/s]\n",
      "Fuzzy Matching Progress: 100%|██████████| 2466/2466 [00:05<00:00, 425.92it/s]\n",
      "Fuzzy Matching Progress: 100%|██████████| 841/841 [00:00<00:00, 1373.43it/s]\n",
      "Fuzzy Matching Progress: 100%|██████████| 334/334 [00:00<00:00, 3429.01it/s]\n",
      "Fuzzy Matching Progress: 100%|██████████| 770/770 [00:00<00:00, 1570.55it/s]\n",
      "Fuzzy Matching Progress: 100%|██████████| 2017/2017 [00:03<00:00, 553.41it/s]\n",
      "Fuzzy Matching Progress: 100%|██████████| 505/505 [00:00<00:00, 1816.62it/s]\n",
      "Fuzzy Matching Progress: 100%|██████████| 2466/2466 [00:07<00:00, 326.91it/s]\n",
      "Fuzzy Matching Progress: 100%|██████████| 85/85 [00:00<00:00, 7563.88it/s]\n",
      "Fuzzy Matching Progress: 100%|██████████| 465/465 [00:00<00:00, 1808.47it/s]\n",
      "Fuzzy Matching Progress: 100%|██████████| 584/584 [00:00<00:00, 1245.87it/s]\n",
      "Fuzzy Matching Progress: 100%|██████████| 21/21 [00:00<00:00, 20827.71it/s]\n",
      "Fuzzy Matching Progress: 100%|██████████| 400/400 [00:00<00:00, 2033.62it/s]\n",
      "Fuzzy Matching Progress: 100%|██████████| 125/125 [00:00<00:00, 6208.85it/s]\n",
      "Fuzzy Matching Progress: 100%|██████████| 248/248 [00:00<00:00, 3358.74it/s]\n",
      "Fuzzy Matching Progress: 100%|██████████| 95/95 [00:00<00:00, 8057.49it/s]\n",
      "Fuzzy Matching Progress: 100%|██████████| 415/415 [00:00<00:00, 1758.63it/s]\n",
      "Fuzzy Matching Progress: 100%|██████████| 33/33 [00:00<00:00, 33002.39it/s]\n",
      "Fuzzy Matching Progress: 100%|██████████| 1222/1222 [00:02<00:00, 479.95it/s]\n",
      "Fuzzy Matching Progress: 100%|██████████| 154/154 [00:00<00:00, 3124.01it/s]\n",
      "Fuzzy Matching Progress: 100%|██████████| 386/386 [00:00<00:00, 2703.27it/s]\n",
      "Fuzzy Matching Progress: 100%|██████████| 163/163 [00:00<00:00, 4429.50it/s]\n",
      "Fuzzy Matching Progress: 100%|██████████| 180/180 [00:00<00:00, 5090.28it/s]\n",
      "Fuzzy Matching Progress: 100%|██████████| 790/790 [00:00<00:00, 1101.40it/s]\n",
      "Fuzzy Matching Progress: 100%|██████████| 769/769 [00:00<00:00, 1165.04it/s]\n",
      "Fuzzy Matching Progress: 100%|██████████| 222/222 [00:00<00:00, 5600.92it/s]\n",
      "Fuzzy Matching Progress: 100%|██████████| 125/125 [00:00<00:00, 7476.37it/s]\n",
      "Fuzzy Matching Progress: 100%|██████████| 17/17 [00:00<00:00, 16888.48it/s]\n",
      "Fuzzy Matching Progress: 100%|██████████| 146/146 [00:00<00:00, 5973.51it/s]\n",
      "Fuzzy Matching Progress: 100%|██████████| 777/777 [00:00<00:00, 1067.58it/s]\n",
      "Fuzzy Matching Progress: 100%|██████████| 195/195 [00:00<00:00, 3702.75it/s]\n",
      "Fuzzy Matching Progress: 100%|██████████| 200/200 [00:00<00:00, 3179.94it/s]\n",
      "Fuzzy Matching Progress: 100%|██████████| 609/609 [00:00<00:00, 1024.41it/s]\n",
      "Fuzzy Matching Progress: 100%|██████████| 10/10 [00:00<00:00, 9906.24it/s]\n",
      "Fuzzy Matching Progress: 100%|██████████| 295/295 [00:00<00:00, 3084.54it/s]\n",
      "Fuzzy Matching Progress: 100%|██████████| 526/526 [00:00<00:00, 1462.84it/s]\n",
      "Fuzzy Matching Progress: 100%|██████████| 68/68 [00:00<00:00, 9190.92it/s]\n",
      "Fuzzy Matching Progress: 100%|██████████| 252/252 [00:00<00:00, 2838.07it/s]\n",
      "Fuzzy Matching Progress: 100%|██████████| 45/45 [00:00<00:00, 16384.00it/s]\n",
      "Fuzzy Matching Progress: 100%|██████████| 607/607 [00:00<00:00, 1341.27it/s]\n",
      "Fuzzy Matching Progress: 100%|██████████| 189/189 [00:00<00:00, 3693.08it/s]\n",
      "Fuzzy Matching Progress: 100%|██████████| 213/213 [00:00<00:00, 4254.77it/s]\n",
      "Fuzzy Matching Progress: 100%|██████████| 128/128 [00:00<00:00, 7593.43it/s]\n",
      "Fuzzy Matching Progress: 100%|██████████| 143/143 [00:00<00:00, 4736.52it/s]\n",
      "Fuzzy Matching Progress: 100%|██████████| 432/432 [00:00<00:00, 1997.77it/s]\n",
      "Fuzzy Matching Progress: 100%|██████████| 784/784 [00:00<00:00, 1096.73it/s]\n",
      "Fuzzy Matching Progress: 100%|██████████| 229/229 [00:00<00:00, 2913.27it/s]\n",
      "Fuzzy Matching Progress: 100%|██████████| 14/14 [00:00<00:00, 13471.04it/s]\n",
      "Fuzzy Matching Progress: 100%|██████████| 123/123 [00:00<00:00, 5228.53it/s]\n",
      "Fuzzy Matching Progress: 100%|██████████| 80/80 [00:00<00:00, 8895.19it/s]\n",
      "Fuzzy Matching Progress: 100%|██████████| 425/425 [00:00<00:00, 1817.19it/s]\n",
      "Fuzzy Matching Progress: 100%|██████████| 245/245 [00:00<00:00, 3072.06it/s]\n",
      "Fuzzy Matching Progress: 100%|██████████| 48/48 [00:00<00:00, 12001.59it/s]\n",
      "Fuzzy Matching Progress: 100%|██████████| 769/769 [00:00<00:00, 828.89it/s]\n",
      "Fuzzy Matching Progress: 100%|██████████| 67/67 [00:00<00:00, 11168.36it/s]\n",
      "Fuzzy Matching Progress: 100%|██████████| 228/228 [00:00<00:00, 4218.58it/s]\n",
      "Fuzzy Matching Progress: 100%|██████████| 97/97 [00:00<00:00, 6444.70it/s]\n",
      "Fuzzy Matching Progress: 100%|██████████| 76/76 [00:00<00:00, 10115.74it/s]\n",
      "Fuzzy Matching Progress: 100%|██████████| 166/166 [00:00<00:00, 4164.15it/s]\n",
      "Fuzzy Matching Progress: 100%|██████████| 192/192 [00:00<00:00, 4066.42it/s]\n",
      "Fuzzy Matching Progress: 100%|██████████| 71/71 [00:00<00:00, 8225.26it/s]\n",
      "Fuzzy Matching Progress: 100%|██████████| 297/297 [00:00<00:00, 1903.76it/s]\n",
      "Fuzzy Matching Progress: 100%|██████████| 474/474 [00:00<00:00, 1560.94it/s]\n",
      "Fuzzy Matching Progress: 100%|██████████| 16/16 [00:00<00:00, 16116.44it/s]\n",
      "Fuzzy Matching Progress: 100%|██████████| 33/33 [00:00<00:00, 16083.20it/s]\n",
      "Fuzzy Matching Progress: 100%|██████████| 55/55 [00:00<00:00, 10905.11it/s]\n",
      "Fuzzy Matching Progress: 100%|██████████| 114/114 [00:00<00:00, 7676.45it/s]\n",
      "Fuzzy Matching Progress: 100%|██████████| 41/41 [00:00<00:00, 16376.20it/s]\n",
      "Fuzzy Matching Progress: 100%|██████████| 197/197 [00:00<00:00, 5135.13it/s]\n",
      "Fuzzy Matching Progress: 100%|██████████| 236/236 [00:00<00:00, 4252.01it/s]\n",
      "Fuzzy Matching Progress: 100%|██████████| 46/46 [00:00<00:00, 18359.31it/s]\n",
      "Fuzzy Matching Progress: 100%|██████████| 24/24 [00:00<00:00, 23361.17it/s]\n",
      "Fuzzy Matching Progress: 100%|██████████| 131/131 [00:00<00:00, 6505.65it/s]\n",
      "Fuzzy Matching Progress: 100%|██████████| 336/336 [00:00<00:00, 2538.49it/s]\n",
      "Fuzzy Matching Progress: 100%|██████████| 305/305 [00:00<00:00, 2522.61it/s]\n",
      "Fuzzy Matching Progress: 100%|██████████| 83/83 [00:00<00:00, 8264.93it/s]\n",
      "Fuzzy Matching Progress: 100%|██████████| 46/46 [00:00<00:00, 13126.82it/s]\n",
      "Fuzzy Matching Progress: 100%|██████████| 293/293 [00:00<00:00, 2416.93it/s]\n",
      "Fuzzy Matching Progress: 100%|██████████| 37/37 [00:00<00:00, 12338.15it/s]\n",
      "Fuzzy Matching Progress: 100%|██████████| 117/117 [00:00<00:00, 5036.47it/s]\n",
      "Fuzzy Matching Progress: 100%|██████████| 123/123 [00:00<00:00, 5548.86it/s]\n",
      "Fuzzy Matching Progress: 100%|██████████| 219/219 [00:00<00:00, 3567.75it/s]\n",
      "Fuzzy Matching Progress: 100%|██████████| 81/81 [00:00<00:00, 10189.20it/s]\n",
      "Fuzzy Matching Progress: 100%|██████████| 169/169 [00:00<00:00, 4949.15it/s]\n",
      "Fuzzy Matching Progress: 100%|██████████| 147/147 [00:00<00:00, 5550.32it/s]\n",
      "Fuzzy Matching Progress: 100%|██████████| 45/45 [00:00<00:00, 21565.78it/s]\n",
      "Fuzzy Matching Progress: 100%|██████████| 115/115 [00:00<00:00, 7662.23it/s]\n",
      "Fuzzy Matching Progress: 100%|██████████| 83/83 [00:00<00:00, 10370.50it/s]\n",
      "Fuzzy Matching Progress: 100%|██████████| 74/74 [00:00<00:00, 9039.19it/s]\n",
      "Fuzzy Matching Progress: 100%|██████████| 58/58 [00:00<00:00, 11557.30it/s]\n",
      "Fuzzy Matching Progress: 100%|██████████| 50/50 [00:00<00:00, 11372.84it/s]\n",
      "Fuzzy Matching Progress: 100%|██████████| 96/96 [00:00<00:00, 7412.07it/s]\n",
      "Fuzzy Matching Progress: 100%|██████████| 108/108 [00:00<00:00, 7444.65it/s]\n",
      "Fuzzy Matching Progress: 100%|██████████| 36/36 [00:00<00:00, 12043.95it/s]\n",
      "Fuzzy Matching Progress: 100%|██████████| 22/22 [00:00<?, ?it/s]\n",
      "Fuzzy Matching Progress: 100%|██████████| 20/20 [00:00<00:00, 20001.45it/s]\n",
      "Fuzzy Matching Progress: 100%|██████████| 167/167 [00:00<00:00, 4996.18it/s]\n",
      "Fuzzy Matching Progress: 100%|██████████| 214/214 [00:00<00:00, 3775.00it/s]\n",
      "Fuzzy Matching Progress: 100%|██████████| 92/92 [00:00<00:00, 10825.83it/s]\n",
      "Fuzzy Matching Progress: 100%|██████████| 56/56 [00:00<00:00, 13989.34it/s]\n",
      "Fuzzy Matching Progress: 100%|██████████| 108/108 [00:00<00:00, 7545.09it/s]\n",
      "Fuzzy Matching Progress: 100%|██████████| 193/193 [00:00<00:00, 4465.79it/s]\n",
      "Fuzzy Matching Progress: 100%|██████████| 193/193 [00:00<00:00, 4285.59it/s]\n",
      "Fuzzy Matching Progress: 100%|██████████| 153/153 [00:00<00:00, 4599.48it/s]\n",
      "Fuzzy Matching Progress: 100%|██████████| 111/111 [00:00<00:00, 6881.50it/s]\n",
      "Fuzzy Matching Progress: 100%|██████████| 71/71 [00:00<00:00, 14064.21it/s]\n",
      "Fuzzy Matching Progress: 100%|██████████| 50/50 [00:00<00:00, 10965.50it/s]\n",
      "Fuzzy Matching Progress: 100%|██████████| 82/82 [00:00<00:00, 8174.28it/s]\n",
      "Fuzzy Matching Progress: 100%|██████████| 46/46 [00:00<00:00, 13001.21it/s]\n",
      "Fuzzy Matching Progress: 100%|██████████| 25/25 [00:00<00:00, 16607.16it/s]\n",
      "Fuzzy Matching Progress: 100%|██████████| 64/64 [00:00<00:00, 9597.95it/s]\n",
      "Fuzzy Matching Progress: 100%|██████████| 184/184 [00:00<00:00, 4138.39it/s]\n",
      "Fuzzy Matching Progress: 100%|██████████| 27/27 [00:00<00:00, 26956.96it/s]\n",
      "Fuzzy Matching Progress: 100%|██████████| 17/17 [00:00<00:00, 10384.96it/s]\n",
      "Fuzzy Matching Progress: 100%|██████████| 71/71 [00:00<00:00, 10876.39it/s]\n",
      "Fuzzy Matching Progress: 100%|██████████| 12/12 [00:00<00:00, 13625.24it/s]\n",
      "Fuzzy Matching Progress: 100%|██████████| 9/9 [00:00<?, ?it/s] 13.78it/s]\n",
      "Fuzzy Matching Progress: 100%|██████████| 4/4 [00:00<00:00, 2579.13it/s]\n",
      "Fuzzy Matching Progress: 100%|██████████| 18/18 [00:00<?, ?it/s]4.28it/s]\n",
      "Fuzzy Matching Progress: 100%|██████████| 19/19 [00:00<?, ?it/s]\n",
      "Fuzzy Matching Progress: 100%|██████████| 15/15 [00:00<00:00, 13897.63it/s]\n",
      "Fuzzy Matching Progress: 100%|██████████| 111/111 [00:00<00:00, 6883.23it/s]\n",
      "Fuzzy Matching Progress: 100%|██████████| 15/15 [00:00<00:00, 14449.83it/s]\n",
      "Fuzzy Matching Progress: 100%|██████████| 41/41 [00:00<00:00, 21015.09it/s]\n",
      "Fuzzy Matching Progress: 100%|██████████| 16/16 [00:00<00:00, 16174.71it/s]\n",
      "Fuzzy Matching Progress: 100%|██████████| 3/3 [00:00<?, ?it/s]\n",
      "Fuzzy Matching Progress: 100%|██████████| 10/10 [00:00<00:00, 9896.89it/s]\n",
      "Fuzzy Matching Progress: 100%|██████████| 16/16 [00:00<00:00, 15985.91it/s]\n",
      "Fuzzy Matching Progress: 100%|██████████| 42/42 [00:00<00:00, 16489.82it/s]\n",
      "Fuzzy Matching Progress: 100%|██████████| 86/86 [00:00<00:00, 10647.01it/s]\n",
      "Fuzzy Matching Progress: 100%|██████████| 9/9 [00:00<?, ?it/s] 16.47it/s]\n",
      "Fuzzy Matching Progress: 100%|██████████| 46/46 [00:00<00:00, 22095.51it/s]\n",
      "Fuzzy Matching Progress: 100%|██████████| 25/25 [00:00<00:00, 25025.68it/s]\n",
      "Fuzzy Matching Progress: 100%|██████████| 48/48 [00:00<00:00, 22815.80it/s]\n",
      "Fuzzy Matching Progress: 100%|██████████| 15/15 [00:00<?, ?it/s]7.14it/s]\n",
      "Fuzzy Matching Progress: 100%|██████████| 5/5 [00:00<?, ?it/s]\n",
      "Fuzzy Matching Progress: 100%|██████████| 46/46 [00:00<00:00, 15514.47it/s]\n",
      "Fuzzy Matching Progress: 100%|██████████| 6/6 [00:00<?, ?it/s]\n",
      "Fuzzy Matching Progress: 100%|██████████| 50/50 [00:00<00:00, 16877.13it/s]\n",
      "Fuzzy Matching Progress: 100%|██████████| 26/26 [00:00<00:00, 26008.09it/s]\n",
      "Fuzzy Matching Progress: 100%|██████████| 54/54 [00:00<00:00, 10796.66it/s]\n",
      "Fuzzy Matching Progress: 100%|██████████| 25/25 [00:00<00:00, 24983.94it/s]\n",
      "Fuzzy Matching Progress: 100%|██████████| 17/17 [00:00<?, ?it/s]7.42it/s]\n",
      "Fuzzy Matching Progress: 100%|██████████| 8/8 [00:00<?, ?it/s]\n",
      "Fuzzy Matching Progress: 100%|██████████| 15/15 [00:00<?, ?it/s]6.70it/s]\n",
      "Fuzzy Matching Progress: 100%|██████████| 6/6 [00:00<?, ?it/s]\n",
      "Fuzzy Matching Progress: 100%|██████████| 14/14 [00:00<00:00, 14459.56it/s]\n",
      "Fuzzy Matching Progress: 100%|██████████| 4/4 [00:00<?, ?it/s]\n",
      "Fuzzy Matching Progress: 100%|██████████| 2/2 [00:00<?, ?it/s] 15.83it/s]\n",
      "Fuzzy Matching Progress: 100%|██████████| 37/37 [00:00<00:00, 14934.97it/s]\n",
      "Fuzzy Matching Progress: 100%|██████████| 12/12 [00:00<00:00, 19886.07it/s]\n",
      "Fuzzy Matching Progress: 100%|██████████| 1/1 [00:00<?, ?it/s]\n",
      "Fuzzy Matching Progress: 100%|██████████| 22/22 [00:00<?, ?it/s]4.90it/s]\n",
      "Fuzzy Matching Progress: 100%|██████████| 15/15 [00:00<?, ?it/s]\n",
      "Fuzzy Matching Progress: 100%|██████████| 1/1 [00:00<?, ?it/s] 14.99it/s]\n",
      "Fuzzy Matching Progress: 100%|██████████| 1/1 [00:00<?, ?it/s]\n",
      "Fuzzy Matching Progress: 100%|██████████| 5/5 [00:00<?, ?it/s] 15.09it/s]\n",
      "Fuzzy Matching Progress: 100%|██████████| 15/15 [00:00<00:00, 14279.29it/s]\n",
      "Fuzzy Matching Progress: 100%|██████████| 4/4 [00:00<?, ?it/s] 15.17it/s]\n",
      "Fuzzy Matching Progress: 100%|██████████| 1/1 [00:00<?, ?it/s]\n",
      "Fuzzy Matching Progress: 100%|██████████| 8/8 [00:00<?, ?it/s] 15.68it/s]\n",
      "Fuzzy Matching Progress: 100%|██████████| 7/7 [00:00<00:00, 6916.40it/s]\n",
      "Fuzzy Matching Progress: 100%|██████████| 5/5 [00:00<?, ?it/s] 15.74it/s]\n",
      "Fuzzy Matching Progress: 100%|██████████| 3/3 [00:00<00:00, 2999.50it/s]\n",
      "Fuzzy Matching Progress: 100%|██████████| 1/1 [00:00<?, ?it/s] 15.76it/s]\n",
      "Fuzzy Matching Progress: 100%|██████████| 1/1 [00:00<?, ?it/s]\n",
      "Fuzzy Matching Progress: 100%|██████████| 1/1 [00:00<00:00, 651.09it/s]s]\n",
      "Fuzzy Matching Progress: 100%|██████████| 3/3 [00:00<?, ?it/s]\n",
      "Fuzzy Matching Progress: 100%|██████████| 1/1 [00:00<?, ?it/s] 16.01it/s]\n",
      "Fuzzy Matching Progress: 100%|██████████| 3/3 [00:00<?, ?it/s]\n",
      "Fuzzy Matching Progress: 100%|██████████| 1/1 [00:00<00:00, 948.51it/s]s]\n",
      "Fuzzy Matching Progress: 100%|██████████| 1/1 [00:00<?, ?it/s]\n",
      "Fuzzy Matching Progress: 100%|██████████| 1/1 [00:00<?, ?it/s]\n",
      "Processing Countries: 100%|██████████| 233/233 [4:28:21<00:00, 69.10s/it]\n"
     ]
    }
   ],
   "source": [
    "# Apply the process to the entire dataset (in my pc it took 268 minutes)\n",
    "final_df = process_all_countries(name_country_data, similarity_threshold=85)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Running the fuzzy matching process on the entire dataset takes approximately 260 minutes. \n",
    "This extended runtime is primarily due to the large size of the dataset,\n",
    "which involves a significant number of string comparisons required to accurately match and assign unique IDs to each firm. \n",
    "Fuzzy matching algorithms, especially those that calculate similarity scores for numerous pairs, are computationally intensive.\n",
    "\n",
    "To manage this, I use a smaller subset of the data for initial testing and debugging purposes. By validating the code on a reduced dataset,\n",
    "we can ensure that the logic and functionality are correct without incurring long processing times.\n",
    "Once the code is verified, it can be efficiently applied to the full dataset, accepting the longer runtime as necessary for comprehensive and accurate matching'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Machine Learning Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Prepare the Data for Training\n",
    "# Identify countries with at least 2 observations for stratification\n",
    "country_counts = final_df['country_iso3'].value_counts()\n",
    "valid_countries = country_counts[country_counts >= 2].index\n",
    "invalid_countries = country_counts[country_counts < 2].index\n",
    "\n",
    "# Set the df\n",
    "filtered_df = final_df[final_df['country_iso3'].isin(valid_countries)].copy()\n",
    "# First divide the dataset into two parts: one for training and one for testing (70% training, 30% testing)\n",
    "train_df, test_df = train_test_split(\n",
    "    filtered_df,\n",
    "    test_size=0.3,\n",
    "    random_state=42,\n",
    "    stratify=filtered_df['country_iso3'] # Stratify by country_iso3 to ensure representation from all countries\n",
    ")\n",
    "\n",
    "# Handle countries with only one observation by adding them to the test set\n",
    "single_observation_df = final_df[final_df['country_iso3'].isin(invalid_countries)].copy()\n",
    "test_df = pd.concat([test_df, single_observation_df], ignore_index=True)\n",
    "\n",
    "# Define a corrections dictionary that includes terms from various countries\n",
    "manual_corrections = {\n",
    "    'technologies': 'tech',\n",
    "    'incorporated': 'inc',\n",
    "    'corporation': 'corp',\n",
    "    'limited': 'ltd',\n",
    "    'llc': '',\n",
    "    'inc': '',\n",
    "    'gmbh': '',\n",
    "    'kg': '',\n",
    "    'sarl': '',\n",
    "}\n",
    "\n",
    "def apply_manual_corrections(name):\n",
    "    words = name.split()\n",
    "    corrected_words = [manual_corrections.get(word.lower(), word) for word in words]\n",
    "    # Remove any empty strings resulting from corrections\n",
    "    corrected_words = [word for word in corrected_words if word]\n",
    "    return ' '.join(corrected_words)\n",
    "\n",
    "# Apply manual corrections to the training data\n",
    "train_df['cleaned_name_manual'] = train_df['cleaned_name'].apply(apply_manual_corrections)\n",
    "\n",
    "## Machine Learning Model\n",
    "\n",
    "# Process the data per country to account for country-specific terms\n",
    "\n",
    "results = []\n",
    "countries = final_df['country_iso3'].unique() # Get the list of unique countries\n",
    "\n",
    "for country in countries:\n",
    "    # Prepare the training data for the current country\n",
    "    train_country_df = train_df[train_df['country_iso3'] == country]\n",
    "    test_country_df = test_df[test_df['country_iso3'] == country]\n",
    "    \n",
    "    if train_country_df.empty or test_country_df.empty:\n",
    "        continue  # Skip if there's no data for this country in either set\n",
    "    \n",
    "    # Prepare the data for training\n",
    "    def prepare_training_data(df):\n",
    "        data = []\n",
    "        for _, row in df.iterrows():\n",
    "            foreign_words = re.findall(r'\\b\\w+\\b', row['foreign'].lower()) # Extract words from the foreign name\n",
    "            cleaned_words = set(row['cleaned_name_manual'].split()) # Extract cleaned words\n",
    "            for word in foreign_words:\n",
    "                label = 1 if word in cleaned_words else 0\n",
    "                data.append({'word': word, 'label': label})\n",
    "        return pd.DataFrame(data)\n",
    "    \n",
    "    training_data = prepare_training_data(train_country_df)\n",
    "    \n",
    "    # Simple feature: word length\n",
    "    training_data['word_length'] = training_data['word'].apply(len)\n",
    "    \n",
    "    # Features and target variable\n",
    "    X_train = training_data[['word_length']]\n",
    "    y_train = training_data['label']\n",
    "    \n",
    "    # Initialize and train the Decision Tree Classifier\n",
    "    model = DecisionTreeClassifier(random_state=42)\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Prepare test data\n",
    "    def prepare_test_data(df):\n",
    "        data = []\n",
    "        index_list = [] \n",
    "        for idx, row in df.iterrows():\n",
    "            foreign_words = re.findall(r'\\b\\w+\\b', row['foreign'].lower())\n",
    "            for word in foreign_words:\n",
    "                data.append({'word': word, 'index': idx})\n",
    "        return pd.DataFrame(data)\n",
    "    \n",
    "    test_data = prepare_test_data(test_country_df)\n",
    "    \n",
    "    # Extract features for test data\n",
    "    test_data['word_length'] = test_data['word'].apply(len)\n",
    "    X_test = test_data[['word_length']]\n",
    "    \n",
    "    # Predict whether to keep each word\n",
    "    test_data['prediction'] = model.predict(X_test)\n",
    "    \n",
    "    # Reconstruct the cleaned names based on predictions\n",
    "    test_country_df = test_country_df.copy()\n",
    "    test_country_df['cleaned_name_ml'] = ''\n",
    "    \n",
    "    for idx in test_country_df.index:\n",
    "        words = test_data[test_data['index'] == idx]\n",
    "        kept_words = words[words['prediction'] == 1]['word']\n",
    "        cleaned_name_ml = ' '.join(kept_words)\n",
    "        test_country_df.at[idx, 'cleaned_name_ml'] = cleaned_name_ml\n",
    "    \n",
    "    # Append the results\n",
    "    results.append(test_country_df)\n",
    "\n",
    "# Combine the results from all countries\n",
    "final_test_df = pd.concat(results, ignore_index=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy After Applying Machine Learning: 0.57\n",
      "\n",
      "Sample Comparison of Cleaned Names:\n",
      "                              foreign                        cleaned_name   \n",
      "0             Pm Piping Singapore Pte                           pm piping  \\\n",
      "1                Premier Sea Land Pte                    premier sea land   \n",
      "2               Seaward Chemicals Pte                   seaward chemicals   \n",
      "3  Panasonic Avionics Corp Sin Branch  panasonic avionics corp sin branch   \n",
      "4            Rs System Technology Pte                rs system technology   \n",
      "\n",
      "                  cleaned_name_ml  \n",
      "0             pm piping singapore  \n",
      "1                    premier land  \n",
      "2               seaward chemicals  \n",
      "3  panasonic avionics corp branch  \n",
      "4            rs system technology  \n"
     ]
    }
   ],
   "source": [
    "# Compare the Accuracy Before and After Applying the ML Model\n",
    "\n",
    "def compare_cleaned_names(row):\n",
    "    return int(row['cleaned_name_ml'] == apply_manual_corrections(row['cleaned_name']))\n",
    "\n",
    "final_test_df['correct_cleaning'] = final_test_df.apply(compare_cleaned_names, axis=1)\n",
    "\n",
    "accuracy = final_test_df['correct_cleaning'].mean()\n",
    "print(f'Accuracy After Applying Machine Learning: {accuracy:.2f}')\n",
    "\n",
    "# Print sample results for inspection\n",
    "print(\"\\nSample Comparison of Cleaned Names:\")\n",
    "print(final_test_df[['foreign', 'cleaned_name', 'cleaned_name_ml']].head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Changed Dataset and Export the Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the columns we are interested in (names_2019_2020 dataset)= ['foreign', 'foreigncountry_cleaned', 'country_iso3', 'cleaned_name', 'cleaned_name_ml']\n",
    "columns = list(names_2019_2020.columns) + ['cleaned_ID', 'cleaned_name']\n",
    "\n",
    "# Set the outputfile_alejandro_1 to save the results\n",
    "outputfile_alejandro_1 = final_df[columns]\n",
    "\n",
    "# Save the results to a CSV file\n",
    "outputfile_alejandro_1.to_csv(\"outputs/outputfile_alejandro_1.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the changes in the naming\n",
    "# Function to standardize strings for comparison\n",
    "def standardize_string(s):\n",
    "    if isinstance(s, str):\n",
    "        return s.lower().strip()\n",
    "    else:\n",
    "        return ''\n",
    "\n",
    "# Apply the function to 'foreign' and 'cleaned_name' variables\n",
    "final_df['foreign_standardized'] = final_df['foreign'].apply(standardize_string)\n",
    "final_df['cleaned_name_standardized'] = final_df['cleaned_name'].apply(standardize_string)\n",
    "\n",
    "# Identify rows where the names have changed\n",
    "outputfile_alejandro_1_changed = final_df[final_df['foreign_standardized'] != final_df['cleaned_name_standardized']].copy()\n",
    "\n",
    "# Include only the original firm name and the cleaned firm name\n",
    "outputfile_alejandro_1_changed = outputfile_alejandro_1_changed[['foreign', 'cleaned_name']]\n",
    "\n",
    "# Save the changed names to a CSV file\n",
    "outputfile_alejandro_1_changed.to_csv(\"outputs/outputfile_alejandro_1_changed.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "_merge\n",
       "both          249777\n",
       "left_only          0\n",
       "right_only         0\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import data\n",
    "names_2021 = pd.read_csv(\"data/ForeignNames_2021.csv\")\n",
    "\n",
    "# Fix Congo in the names_2021 dataset and apply manual mapping\n",
    "names_2021['foreigncountry_cleaned'] = names_2021['foreigncountry_cleaned'].replace('Congo, the Democratic Republic of the','Congo')\n",
    "\n",
    "# Apply the manual mapping to the 'foreigncountry_cleaned' column\n",
    "names_2021['name_clean'] = names_2021['foreigncountry_cleaned'].replace(manual_country_mapping)\n",
    "\n",
    "# Now we can merge based on the mapped country names\n",
    "names_2021 = pd.merge(names_2021, country_iso, \n",
    "                      left_on='foreigncountry_cleaned', \n",
    "                      right_on='name_clean', \n",
    "                      how='left', indicator=True)\n",
    "\n",
    "names_2021._merge.value_counts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the rows where the company name is missing\n",
    "names_2021 = names_2021.dropna(subset=['foreign'])\n",
    "# Apply the cleaning function\n",
    "names_2021['cleaned_name'] = names_2021.apply(\n",
    "    lambda row: clean_company_name(\n",
    "        row['foreign'],\n",
    "        countries_stopwords.get(row['country_iso3'], [])\n",
    "    ),\n",
    "    axis=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the old data, data from 2019-2020\n",
    "old_data = final_df[['cleaned_name', 'cleaned_ID', 'country_iso3', 'foreign']].copy()\n",
    "old_data = old_data.drop_duplicates(subset=['cleaned_name', 'country_iso3'])\n",
    "\n",
    "# Prepare the new data, data from 2021\n",
    "new_data = names_2021[['foreign', 'cleaned_name', 'country_iso3']].copy()\n",
    "\n",
    "# Set 'cleaned_ID', 'new', and 'old_name'\n",
    "new_data['cleaned_ID'] = np.nan\n",
    "new_data['new'] = 1  # Assume new firms initially\n",
    "new_data['old_name'] = np.nan\n",
    "\n",
    "# Function to match and assign IDs\n",
    "def match_and_assign_ids(row):\n",
    "    country = row['country_iso3']\n",
    "    cleaned_name = row['cleaned_name']\n",
    "    \n",
    "    # Filter old data for the same country\n",
    "    old_data_country = old_data[old_data['country_iso3'] == country]\n",
    "    \n",
    "    if old_data_country.empty:\n",
    "        return row  # No old data for this country\n",
    "    \n",
    "    # Use RapidFuzz to find the best match\n",
    "    match = process.extractOne(\n",
    "        cleaned_name,\n",
    "        old_data_country['cleaned_name'],\n",
    "        scorer=fuzz.token_sort_ratio, # Use token_sort_ratio for better matching\n",
    "        score_cutoff=85  # Similarity threshold\n",
    "    )\n",
    "    \n",
    "    if match:\n",
    "        # Match found\n",
    "        matched_cleaned_name = match[0]\n",
    "        matched_row = old_data_country[old_data_country['cleaned_name'] == matched_cleaned_name].iloc[0] # Get the first match\n",
    "        row['cleaned_ID'] = matched_row['cleaned_ID']\n",
    "        row['new'] = 0\n",
    "        row['old_name'] = matched_row['foreign']\n",
    "    else:\n",
    "        # No match found, assign new ID\n",
    "        max_id = old_data_country['cleaned_ID'].str.extract(rf'{country}_(\\d+)')[0].astype(float).max() # Extract the ID number and get the max\n",
    "        if pd.isna(max_id):\n",
    "            max_id = 0\n",
    "        new_id_number = int(max_id) + 1\n",
    "        row['cleaned_ID'] = f\"{country}_{new_id_number}\"\n",
    "    return row\n",
    "\n",
    "# Apply the function row-wise\n",
    "new_data = new_data.apply(match_and_assign_ids, axis=1)\n",
    "\n",
    "# Select and reorder the required columns\n",
    "outputfile_alejandro_2 = new_data[['foreign', 'cleaned_name', 'cleaned_ID', 'new', 'old_name']]\n",
    "\n",
    "# Export the results to a CSV file\n",
    "outputfile_alejandro_2.to_csv(\"outputs/outputfile_alejandro_2.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
