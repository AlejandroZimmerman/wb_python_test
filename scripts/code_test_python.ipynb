{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Research Assistant Python Test**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import packages needed for the test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Import packages\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import regex as re\n",
    "from collections import Counter\n",
    "from tqdm import tqdm\n",
    "from rapidfuzz import process, fuzz\n",
    "\n",
    "# ML packages\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.tree import DecisionTreeClassifier\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Import data\n",
    "# Set the directory path\n",
    "directory_path = r\"C:\\Users\\alezi\\OneDrive\\Escritorio\\Test WB\\Python\"\n",
    "# directory_path = r\"WB_Path\"\n",
    "\n",
    "# Change the current working directory\n",
    "os.chdir(directory_path)\n",
    "\n",
    "# Load the data\n",
    "names_2019_2020 = pd.read_csv(\"data/ForeignNames_2019_2020.csv\")\n",
    "country_iso = pd.read_csv(\"data/Country_Name_ISO3.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge the foreign names dataset with the country ISO3 codes\n",
    "name_country_data = pd.merge(names_2019_2020 , country_iso, \n",
    "                     left_on='foreigncountry_cleaned', \n",
    "                     right_on='country_name', \n",
    "                     how='left', indicator=True)\n",
    "\n",
    "# Check the merge results\n",
    "merge_result_counts = name_country_data['_merge'].value_counts()\n",
    "print(\"Merge Results (Value Counts):\")\n",
    "print(merge_result_counts)\n",
    "print(\"\\n\") \n",
    "# There are observations that did not merge (left_only)\n",
    "\n",
    "# Check the countries that did not merge(South Korea, North Korea, Iran, Congo, the Democratic Republic of the, Antigua And Barbuda)\n",
    "print(\"Countries that did not merge (left_only):\")\n",
    "unmerged_countries = name_country_data[name_country_data['_merge'] == 'left_only']['foreigncountry_cleaned'].value_counts()\n",
    "print(unmerged_countries)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Fixing the Merge \n",
    "\n",
    "# Fix Congo in the names_2019_2020 dataset\n",
    "names_2019_2020['foreigncountry_cleaned'] = names_2019_2020['foreigncountry_cleaned'].replace('Congo, the Democratic Republic of the','Congo')\n",
    "\n",
    "# Manual mapping of problematic country names\n",
    "manual_country_mapping = {\n",
    "    'Korea, Republic of': 'South Korea',\n",
    "    'Iran, Islamic Republic of': 'Iran', \n",
    "    'Congo, The Democratic Republic of the': 'Congo',\n",
    "    'Antigua and Barbuda': 'Antigua And Barbuda',\n",
    "    'Tanzania, United Republic of': 'Tanzania',\n",
    "}\n",
    "\n",
    "# Apply the manual mapping to the 'foreigncountry_cleaned' column\n",
    "country_iso['name_clean'] = country_iso['country_name'].replace(manual_country_mapping)\n",
    "\n",
    "# Now we can merge based on the mapped country names\n",
    "name_country_data = pd.merge(names_2019_2020, country_iso, \n",
    "                             left_on='foreigncountry_cleaned', \n",
    "                             right_on='name_clean', \n",
    "                             how='left', indicator=True)\n",
    "\n",
    "# Check the results\n",
    "merge_result_counts = name_country_data['_merge'].value_counts()\n",
    "print(\"Merge Results (Value Counts):\")\n",
    "print(merge_result_counts)\n",
    "\n",
    "# Print that if there are no left_only values (left_only = 0), the merge was successful\n",
    "if merge_result_counts['left_only'] == 0:\n",
    "    print(\"\\n\")\n",
    "    print(\"Merge was successful\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Cleaning and Matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "## STOPWORDS PER COUNTRY ##\n",
    "\n",
    "'''\n",
    "Define stopwords for cleaning, customized per country.\n",
    "\n",
    "To enhance the accuracy of cleaning and matching company names, I process the dataset country by country.\n",
    "For each country, I select the top 10 most frequent words found in the company names specific to that country.\n",
    "These words are added to a dictionary called 'countries_stopwords', where each key is a country ISO3 code, \n",
    "and the value is the list of stopwords for that country.\n",
    "\n",
    "As I analyze more data, I can expand and refine the stopword lists for each country to further enhance accuracy.\n",
    "'''\n",
    "\n",
    "\n",
    "# Initialize an empty dictionary to hold stopwords per country\n",
    "countries_stopwords = {}\n",
    "\n",
    "# Group the data by 'country_iso3'\n",
    "grouped = name_country_data.groupby('country_iso3')\n",
    "\n",
    "# Iterate over each country group\n",
    "for country_code, group in grouped:\n",
    "    # Get the list of company names for this country, ensuring no NaN values\n",
    "    company_names = group['foreign'].dropna().tolist()\n",
    "    \n",
    "    # Initialize a list to hold all words\n",
    "    all_words = []\n",
    "    \n",
    "    # For each company name, extract words\n",
    "    for name in company_names:\n",
    "        # Convert to lowercase and remove non-alphabetic characters\n",
    "        name = name.lower()\n",
    "        name = re.sub(r'[^a-z\\s]', '', name)\n",
    "        # Split into words\n",
    "        words = name.split()\n",
    "        # Add words to the list\n",
    "        all_words.extend(words)\n",
    "    \n",
    "    # Count the frequency of each word\n",
    "    word_counts = Counter(all_words)\n",
    "    # Get the top 8 most common words\n",
    "    top_words = [word for word, count in word_counts.most_common(8)]\n",
    "    # Add these words to the countries_stopwords dictionary\n",
    "    countries_stopwords[country_code] = top_words\n",
    "    \n",
    "# Add manuallly defined stopwords \n",
    "countries_stopwords['USA'].append('technologies')\n",
    "countries_stopwords['CHN'].append('machinery')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the function to clean the company names\n",
    "def clean_company_name(company_name, stopwords):\n",
    "    if not isinstance(company_name, str):\n",
    "        return ''\n",
    "    # Convert to lowercase\n",
    "    company_name = company_name.lower()\n",
    "    # Remove non-alphabetic characters\n",
    "    company_name = re.sub(r'[^a-z\\s]', '', company_name)\n",
    "    # Split into words\n",
    "    words = company_name.split()\n",
    "    # Remove stopwords\n",
    "    cleaned_words = [word for word in words if word not in stopwords]\n",
    "    # Reconstruct the cleaned name\n",
    "    cleaned_name = ' '.join(cleaned_words)\n",
    "    return cleaned_name\n",
    "\n",
    "# Drop the rows where the company name is missing\n",
    "name_country_data = name_country_data.dropna(subset=['foreign'])\n",
    "if name_country_data['foreign'].isnull().sum() == 0:\n",
    "    print(\"There are no missing values in the 'foreign' column\")\n",
    "\n",
    "# Apply the cleaning function to the DataFrame using country-specific stopwords\n",
    "name_country_data['cleaned_name'] = name_country_data.apply(\n",
    "    lambda row: clean_company_name(\n",
    "        row['foreign'],\n",
    "        countries_stopwords.get(row['country_iso3'], [])\n",
    "    ),\n",
    "    axis=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "## RapidFuzz Functions ##\n",
    "\n",
    "# Function to apply fuzzy matching within the same country using RapidFuzz\n",
    "def fuzzy_match_within_country(df, similarity_threshold=85, min_length=4):\n",
    "    # Use unique cleaned names to avoid duplicates\n",
    "    unique_cleaned_names = list(set(df['cleaned_name'].tolist()))\n",
    "    name_to_group = {}\n",
    "    group_id = 0\n",
    "    processed = set()\n",
    "\n",
    "    for name in tqdm(unique_cleaned_names, desc=\"Fuzzy Matching Progress\"):\n",
    "        if name in processed:\n",
    "            continue\n",
    "\n",
    "        # Assign unique group_id to short names (in this way I avoid that all short names are assigned to the same group)\n",
    "        if len(name) < min_length:\n",
    "            name_to_group[name] = group_id\n",
    "            processed.add(name)\n",
    "            group_id += 1\n",
    "            continue\n",
    "\n",
    "        # Use token_sort_ratio for matching longer names and prevent false positives (e.g., \"tti\" matches \"patiperro\" in DEU)\n",
    "        matches = process.extract(\n",
    "            name,\n",
    "            unique_cleaned_names,\n",
    "            scorer=fuzz.token_sort_ratio,\n",
    "            score_cutoff=similarity_threshold,\n",
    "            limit=None\n",
    "        )\n",
    "\n",
    "        similar_names = [match[0] for match in matches if len(match[0]) >= min_length] # Filter out short names\n",
    "\n",
    "        if not similar_names:\n",
    "            name_to_group[name] = group_id\n",
    "            processed.add(name)\n",
    "            group_id += 1\n",
    "            continue\n",
    "\n",
    "        # Assign group_id to all similar names\n",
    "        for sim_name in similar_names:\n",
    "            if sim_name not in name_to_group:\n",
    "                name_to_group[sim_name] = group_id\n",
    "                processed.add(sim_name)\n",
    "        group_id += 1\n",
    "\n",
    "    # Map group_id back to the DataFrame\n",
    "    df['group_id'] = df['cleaned_name'].map(name_to_group)\n",
    "    return df\n",
    "\n",
    "\n",
    "# Function to assign cleaned_ID based on group_id and country code\n",
    "def assign_cleaned_id(df, country_code):\n",
    "    df['cleaned_ID'] = country_code + '_' + df['group_id'].astype(str)\n",
    "    return df\n",
    "\n",
    "def process_all_countries(df, similarity_threshold=85):\n",
    "    final_dfs = []\n",
    "    countries = df['country_iso3'].unique()\n",
    "    \n",
    "    for country in tqdm(countries, desc=\"Processing Countries\"):\n",
    "        country_df = df[df['country_iso3'] == country].reset_index(drop=True)\n",
    "        if country_df.empty:\n",
    "            continue\n",
    "        # Apply fuzzy matching within the country\n",
    "        matched_df = fuzzy_match_within_country(country_df, similarity_threshold)\n",
    "        # Assign cleaned IDs\n",
    "        assigned_df = assign_cleaned_id(matched_df, country)\n",
    "        final_dfs.append(assigned_df)\n",
    "    \n",
    "    # Combine all country DataFrames\n",
    "    final_df = pd.concat(final_dfs, ignore_index=True)\n",
    "    return final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the process to the entire dataset\n",
    "final_df = process_all_countries(name_country_data, similarity_threshold=85)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Machine Learning Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and testing sets\n",
    "train_df, test_df = train_test_split(final_df, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Prepare the Data for Training\n",
    "# First divide the dataset into two parts: one for training and one for testing (70% training, 30% testing)\n",
    "train_df, test_df = train_test_split(\n",
    "    final_df,\n",
    "    test_size=0.3,\n",
    "    random_state=42,\n",
    "    stratify=final_df['country_iso3'] # Stratify by country_iso3 to ensure representation from all countries\n",
    ")\n",
    "\n",
    "# Define a corrections dictionary that includes terms from various countries\n",
    "manual_corrections = {\n",
    "    'technologies': 'tech',\n",
    "    'incorporated': 'inc',\n",
    "    'corporation': 'corp',\n",
    "    'limited': 'ltd',\n",
    "    'llc': '',\n",
    "    'inc': '',\n",
    "    'gmbh': '',\n",
    "    'kg': '',\n",
    "    'sarl': '',\n",
    "}\n",
    "\n",
    "def apply_manual_corrections(name):\n",
    "    words = name.split()\n",
    "    corrected_words = [manual_corrections.get(word.lower(), word) for word in words]\n",
    "    # Remove any empty strings resulting from corrections\n",
    "    corrected_words = [word for word in corrected_words if word]\n",
    "    return ' '.join(corrected_words)\n",
    "\n",
    "# Apply manual corrections to the training data\n",
    "train_df['cleaned_name_manual'] = train_df['cleaned_name'].apply(apply_manual_corrections)\n",
    "\n",
    "## Machine Learning Model\n",
    "\n",
    "# Process the data per country to account for country-specific terms\n",
    "\n",
    "results = []\n",
    "countries = final_df['country_iso3'].unique() # Get the list of unique countries\n",
    "\n",
    "for country in countries:\n",
    "    # Prepare the training data for the current country\n",
    "    train_country_df = train_df[train_df['country_iso3'] == country]\n",
    "    test_country_df = test_df[test_df['country_iso3'] == country]\n",
    "    \n",
    "    if train_country_df.empty or test_country_df.empty:\n",
    "        continue  # Skip if there's no data for this country in either set\n",
    "    \n",
    "    # Prepare the data for training\n",
    "    def prepare_training_data(df):\n",
    "        data = []\n",
    "        for _, row in df.iterrows():\n",
    "            foreign_words = re.findall(r'\\b\\w+\\b', row['foreign'].lower()) # Extract words from the foreign name\n",
    "            cleaned_words = set(row['cleaned_name_manual'].split()) # Extract cleaned words\n",
    "            for word in foreign_words:\n",
    "                label = 1 if word in cleaned_words else 0\n",
    "                data.append({'word': word, 'label': label})\n",
    "        return pd.DataFrame(data)\n",
    "    \n",
    "    training_data = prepare_training_data(train_country_df)\n",
    "    \n",
    "    # Simple feature: word length\n",
    "    training_data['word_length'] = training_data['word'].apply(len)\n",
    "    \n",
    "    # Features and target variable\n",
    "    X_train = training_data[['word_length']]\n",
    "    y_train = training_data['label']\n",
    "    \n",
    "    # Initialize and train the Decision Tree Classifier\n",
    "    model = DecisionTreeClassifier(random_state=42)\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Prepare test data\n",
    "    def prepare_test_data(df):\n",
    "        data = []\n",
    "        index_list = [] \n",
    "        for idx, row in df.iterrows():\n",
    "            foreign_words = re.findall(r'\\b\\w+\\b', row['foreign'].lower())\n",
    "            for word in foreign_words:\n",
    "                data.append({'word': word, 'index': idx})\n",
    "        return pd.DataFrame(data)\n",
    "    \n",
    "    test_data = prepare_test_data(test_country_df)\n",
    "    \n",
    "    # Extract features for test data\n",
    "    test_data['word_length'] = test_data['word'].apply(len)\n",
    "    X_test = test_data[['word_length']]\n",
    "    \n",
    "    # Predict whether to keep each word\n",
    "    test_data['prediction'] = model.predict(X_test)\n",
    "    \n",
    "    # Reconstruct the cleaned names based on predictions\n",
    "    test_country_df = test_country_df.copy()\n",
    "    test_country_df['cleaned_name_ml'] = ''\n",
    "    \n",
    "    for idx in test_country_df.index:\n",
    "        words = test_data[test_data['index'] == idx]\n",
    "        kept_words = words[words['prediction'] == 1]['word']\n",
    "        cleaned_name_ml = ' '.join(kept_words)\n",
    "        test_country_df.at[idx, 'cleaned_name_ml'] = cleaned_name_ml\n",
    "    \n",
    "    # Append the results\n",
    "    results.append(test_country_df)\n",
    "\n",
    "# Combine the results from all countries\n",
    "final_test_df = pd.concat(results, ignore_index=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare the Accuracy Before and After Applying the ML Model\n",
    "\n",
    "def compare_cleaned_names(row):\n",
    "    return int(row['cleaned_name_ml'] == apply_manual_corrections(row['cleaned_name']))\n",
    "\n",
    "final_test_df['correct_cleaning'] = final_test_df.apply(compare_cleaned_names, axis=1)\n",
    "\n",
    "accuracy = final_test_df['correct_cleaning'].mean()\n",
    "print(f'Accuracy After Applying Machine Learning: {accuracy:.2f}')\n",
    "\n",
    "# Print sample results for inspection\n",
    "print(\"\\nSample Comparison of Cleaned Names:\")\n",
    "print(final_test_df[['foreign', 'cleaned_name', 'cleaned_name_ml']].head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Changed Dataset and Export the Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the columns we are interested in (names_2019_2020 dataset)= ['foreign', 'foreigncountry_cleaned', 'country_iso3', 'cleaned_name', 'cleaned_name_ml']\n",
    "columns = list(names_2019_2020.columns) + ['cleaned_ID', 'cleaned_name']\n",
    "\n",
    "# Set the outputfile_alejandro_1 to save the results\n",
    "outputfile_alejandro_1 = final_df[columns]\n",
    "\n",
    "# Save the results to a CSV file\n",
    "outputfile_alejandro_1.to_csv(\"output/outputfile_alejandro_1.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the changes in the naming\n",
    "# Function to standardize strings for comparison\n",
    "def standardize_string(s):\n",
    "    if isinstance(s, str):\n",
    "        return s.lower().strip()\n",
    "    else:\n",
    "        return ''\n",
    "\n",
    "# Apply the function to 'foreign' and 'cleaned_name' variables\n",
    "final_df['foreign_standardized'] = final_df['foreign'].apply(standardize_string)\n",
    "final_df['cleaned_name_standardized'] = final_df['cleaned_name'].apply(standardize_string)\n",
    "\n",
    "# Identify rows where the names have changed\n",
    "outputfile_alejandro_1_changed = final_df[final_df['foreign_standardized'] != final_df['cleaned_name_standardized']].copy()\n",
    "\n",
    "# Include only the original firm name and the cleaned firm name\n",
    "outputfile_alejandro_1_changed = outputfile_alejandro_1_changed[['foreign', 'cleaned_name']]\n",
    "\n",
    "# Save the changed names to a CSV file\n",
    "outputfile_alejandro_1_changed.to_csv(\"output/outputfile_alejandro_1_changed.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import data\n",
    "names_2021 = pd.read_csv(\"data/ForeignNames_2021.csv\")\n",
    "\n",
    "# Fix Congo in the names_2021 dataset and apply manual mapping\n",
    "names_2021['foreigncountry_cleaned'] = names_2021['foreigncountry_cleaned'].replace('Congo, the Democratic Republic of the','Congo')\n",
    "\n",
    "# Apply the manual mapping to the 'foreigncountry_cleaned' column\n",
    "names_2021['name_clean'] = names_2021['foreigncountry_cleaned'].replace(manual_country_mapping)\n",
    "\n",
    "# Now we can merge based on the mapped country names\n",
    "names_2021 = pd.merge(names_2021, country_iso, \n",
    "                      left_on='foreigncountry_cleaned', \n",
    "                      right_on='name_clean', \n",
    "                      how='left', indicator=True)\n",
    "\n",
    "names_2021._merge.value_counts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the rows where the company name is missing\n",
    "names_2021 = names_2021.dropna(subset=['foreign'])\n",
    "# Apply the cleaning function\n",
    "names_2021['cleaned_name'] = names_2021.apply(\n",
    "    lambda row: clean_company_name(\n",
    "        row['foreign'],\n",
    "        countries_stopwords.get(row['country_iso3'], [])\n",
    "    ),\n",
    "    axis=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the old data, data from 2019-2020\n",
    "old_data = final_df[['cleaned_name', 'cleaned_ID', 'country_iso3', 'foreign']].copy()\n",
    "old_data = old_data.drop_duplicates(subset=['cleaned_name', 'country_iso3'])\n",
    "\n",
    "# Prepare the new data, data from 2021\n",
    "new_data = names_2021[['foreign', 'cleaned_name', 'country_iso3']].copy()\n",
    "\n",
    "# Set 'cleaned_ID', 'new', and 'old_name'\n",
    "new_data['cleaned_ID'] = np.nan\n",
    "new_data['new'] = 1  # Assume new firms initially\n",
    "new_data['old_name'] = np.nan\n",
    "\n",
    "# Function to match and assign IDs\n",
    "def match_and_assign_ids(row):\n",
    "    country = row['country_iso3']\n",
    "    cleaned_name = row['cleaned_name']\n",
    "    \n",
    "    # Filter old data for the same country\n",
    "    old_data_country = old_data[old_data['country_iso3'] == country]\n",
    "    \n",
    "    if old_data_country.empty:\n",
    "        return row  # No old data for this country\n",
    "    \n",
    "    # Use RapidFuzz to find the best match\n",
    "    match = process.extractOne(\n",
    "        cleaned_name,\n",
    "        old_data_country['cleaned_name'],\n",
    "        scorer=fuzz.token_sort_ratio, # Use token_sort_ratio for better matching\n",
    "        score_cutoff=85  # Similarity threshold\n",
    "    )\n",
    "    \n",
    "    if match:\n",
    "        # Match found\n",
    "        matched_cleaned_name = match[0]\n",
    "        matched_row = old_data_country[old_data_country['cleaned_name'] == matched_cleaned_name].iloc[0] # Get the first match\n",
    "        row['cleaned_ID'] = matched_row['cleaned_ID']\n",
    "        row['new'] = 0\n",
    "        row['old_name'] = matched_row['foreign']\n",
    "    else:\n",
    "        # No match found, assign new ID\n",
    "        max_id = old_data_country['cleaned_ID'].str.extract(rf'{country}_(\\d+)')[0].astype(float).max() # Extract the ID number and get the max\n",
    "        if pd.isna(max_id):\n",
    "            max_id = 0\n",
    "        new_id_number = int(max_id) + 1\n",
    "        row['cleaned_ID'] = f\"{country}_{new_id_number}\"\n",
    "    return row\n",
    "\n",
    "# Apply the function row-wise\n",
    "new_data = new_data.apply(match_and_assign_ids, axis=1)\n",
    "\n",
    "# Select and reorder the required columns\n",
    "outputfile_alejandro_2 = new_data[['foreign', 'cleaned_name', 'cleaned_ID', 'new', 'old_name']]\n",
    "\n",
    "# Export the results to a CSV file\n",
    "outputfile_alejandro_2.to_csv(\"output/outputfile_alejandro_2.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
